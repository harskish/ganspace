{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ganspace_colab.ipynb","provenance":[{"file_id":"https://github.com/sdtblck/ganspace/blob/master/notebooks/Ganspace_colab.ipynb","timestamp":1589390451074}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0sPTnpX6JcrU","colab_type":"text"},"source":["#GANSPACE - Discovering Interpretable GAN Controls \n","\n","Using https://github.com/harskish/ganspace to find latent directions in a stylegan2 model. (This could easily be ported to other models, if anyone implements it please get in touch, and i'll add it to the notebook!)\n","\n","Notebook put together by [@realmeatyhuman](https://twitter.com/realmeatyhuman)\n","\n"]},{"cell_type":"code","metadata":{"id":"yyjPqusnkHHJ","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lM2k1EDpBZFc","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-gc95AN_wQ4","colab_type":"text"},"source":["\n","## Setup\n","It's quite a long install - but if you hit play on all the cells below, everything should run smoothly. Go grab yourself a cup of tea.\n","\n"]},{"cell_type":"code","metadata":{"id":"04xop1hZISlG","colab_type":"code","cellView":"both","colab":{}},"source":["# Clone git\n","!git clone https://github.com/harskish/ganspace\n","%cd ganspace\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKCsAiNNFLnn","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Install remaining packages\n","!pip install fbpca pyopengltk Ninja\n","!git submodule update --init --recursive\n","!python -c \"import nltk; nltk.download('wordnet')\"\n","%cd models/stylegan2/stylegan2-pytorch/op\n","!python setup.py install\n","!python -c \"import torch; import upfirdn2d_op; import fused; print('OK')\"\n","%cd \"/content/ganspace\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jo8zwRspKJBb","colab_type":"text"},"source":["# Convert model weights\n","\n","If you have a tensorflow model you want to use Ganspace on - convert it to a pytorch model below.\n","\n","(skip this step if you already have a pytorch model)"]},{"cell_type":"code","metadata":{"id":"0c32G3UvnFaV","colab_type":"code","colab":{}},"source":["!gdown --id 1UlDmJVLLnBD9SnLSMXeiZRO6g-OMQCA_ -O /content/ffhq.pkl"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQcivtikdOsC","colab_type":"code","colab":{}},"source":["%cd \"/content\"\n","!git clone https://github.com/skyflynil/stylegan2\n","%cd ganspace"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQPsNKq9n2pI","colab_type":"text"},"source":["The convert weight script takes two arguments: \n","\n","```\n","--repo - Path to tensorflow stylegan2 repo\n","       - Path to your model\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"Pdkk4vyXtrNh","colab_type":"code","colab":{}},"source":["!python /content/ganspace/models/stylegan2/stylegan2-pytorch/convert_weight.py --repo=\"/content/stylegan2/\" \"/content/ffhq.pkl\" #convert weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pRwkojK1uP-","colab_type":"code","colab":{}},"source":["!cp \"/content/ganspace/ffhq.pt\" \"/content/drive/My Drive/ML/stylegan_models\" #copy pytorch model to your drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhVTzEZjrdFv","colab_type":"text"},"source":["# Run PCA Analysis"]},{"cell_type":"markdown","metadata":{"id":"MRuvtoQhrt1J","colab_type":"text"},"source":["From here, open models/wrappers.py, and edit the stylegan2 configs dict on line 110 to include your model and its corresponding resolution.\n","\n","I.E from\n","\n","        # Image widths\n","        configs = {\n","            # Converted NVIDIA official\n","            'ffhq': 1024,\n","            'car': 512,\n","            'cat': 256,\n","            'church': 256,\n","            'horse': 256,\n","            # Tuomas\n","            'bedrooms': 256,\n","            'kitchen': 256,\n","            'places': 256,\n","        }\n","\n","to \n","\n","        # Image widths\n","        configs = {\n","            # Converted NVIDIA official\n","            'your_model': your_resolution,\n","            'ffhq': 1024,\n","            'car': 512,\n","            'cat': 256,\n","            'church': 256,\n","            'horse': 256,\n","            # Tuomas\n","            'bedrooms': 256,\n","            'kitchen': 256,\n","            'places': 256,\n","        }\n","\n","Then copy your pytorch model over to your drive account or any other hosting platform, and add the direct download link to the checkpoints dict in the download_checkpoint function on line 136.\n","\n","    def download_checkpoint(self, outfile):\n","        checkpoints = { 'yourmodel': 'https://drive.google.com/yourmodel',\n","            'horse': 'https://drive.google.com/uc?export=download&id=18SkqWAkgt0fIwDEf2pqeaenNi4OoCo-0',\n","            'ffhq': 'https://drive.google.com/uc?id=12yYXZymadSIj74Yue1Q7RrlbIqrXggo3',\n","            'church': 'https://drive.google.com/uc?export=download&id=1HFM694112b_im01JT7wop0faftw9ty5g',\n","            'car': 'https://drive.google.com/uc?export=download&id=1iRoWclWVbDBAy5iXYZrQnKYSbZUqXI6y',\n","            'cat': 'https://drive.google.com/uc?export=download&id=15vJP8GDr0FlRYpE8gD7CdeEz2mXrQMgN',\n","            'places': 'https://drive.google.com/uc?export=download&id=1X8-wIH3aYKjgDZt4KMOtQzN1m4AlCVhm',\n","            'bedrooms': 'https://drive.google.com/uc?export=download&id=1nZTW7mjazs-qPhkmbsOLLA_6qws-eNQu',\n","            'kitchen': 'https://drive.google.com/uc?export=download&id=15dCpnZ1YLAnETAPB0FGmXwdBclbwMEkZ'\n","        }\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7GggN36LpSgo","colab_type":"text"},"source":["##Options\n","\n","\n","```\n","Command line paramaters:\n","  --model      one of [ProGAN, BigGAN-512, BigGAN-256, BigGAN-128, StyleGAN, StyleGAN2]\n","  --class      class name; leave empty to list options\n","  --layer      layer at which to perform PCA; leave empty to list options\n","  --use_w      treat W as the main latent space (StyleGAN / StyleGAN2)\n","  --inputs     load previously exported edits from directory\n","  --sigma      number of stdevs to use in visualize.py\n","  -n           number of PCA samples\n","  -b           override automatic minibatch size detection\n","  -c           number of components to keep\n","\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AGTY0XmaIfz_","colab_type":"code","colab":{}},"source":["%cd ../ganspace/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AglcMDAUt-mm","colab_type":"code","colab":{}},"source":["model = 'StyleGAN2' \n","model_class = 'ffhq' #this is the name of your model in the configs\n","num_components = 80"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RmHtfgdomsx","colab_type":"code","cellView":"both","colab":{}},"source":["#Check layers available for analysis\n","!python visualize.py --model $model --class $model_class --use_w"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iwoyJLamjciq","colab_type":"text"},"source":["Add chosen layer in as --layer argument:"]},{"cell_type":"code","metadata":{"id":"D41v8w25l0j8","colab_type":"code","colab":{}},"source":["!python visualize.py --model $model --class $model_class --use_w --layer=style -c $num_components"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uSCkh4OAqOU","colab_type":"code","colab":{}},"source":["!python visualize.py --model=$model --class=$model_class --use_w --layer=\"style\" -b=500 --batch --video #add -video to generate videos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Vf0BDQMIVPi","colab_type":"code","colab":{}},"source":["!python visualize.py --model=StyleGAN2 --class=ffhq --use_w --layer=style -b=10000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JW_I1n1DMMEO","colab_type":"code","colab":{}},"source":["!zip -r samples.zip \"/content/ganspace/out/StyleGAN2-ffhq\" #zip up samples for download"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9Oz_9TjuBPc","colab_type":"code","colab":{}},"source":["%cp -r \"/content/ganspace/cache/components\" \"/content/drive/My Drive/ML/stylegan2/comps\" #copying components over to google drive"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7TDzlXCHe59","colab_type":"text"},"source":["# Explore Directions!"]},{"cell_type":"markdown","metadata":{"id":"Y9_fWV-NpuLA","colab_type":"text"},"source":["After running visualize.py, your components will be stored in an npz file in /content/ganspace/cache/components/ - below the npz file is unpacked, and a component/direction is chosen at random. \n","\n","Using the UI, you can explore the latent direction and give it a name, which will be appeneded to the named_directions dictionary and saved as 'direction_name.npy' for later use.\n","\n","From here, you may want to copy the components over to your drive, factory reset the notebook and reinstall tensorflow. I'm getting some errors with conflicting versions.\n"]},{"cell_type":"code","metadata":{"id":"rD24fpCcHnyV","colab_type":"code","colab":{}},"source":["# Load model\n","from IPython.utils import io\n","import torch\n","import PIL\n","import numpy as np\n","import ipywidgets as widgets\n","from PIL import Image\n","import imageio\n","from models import get_instrumented_model\n","from decomposition import get_or_compute\n","from config import Config\n","from skimage import img_as_ubyte\n","\n","# Speed up computation\n","torch.autograd.set_grad_enabled(False)\n","torch.backends.cudnn.benchmark = True\n","\n","# Specify model to use\n","config = Config(\n","  model='StyleGAN2',\n","  layer='style',\n","  output_class=model_class,\n","  components=num_components,\n","  use_w=True\n",")\n","\n","inst = get_instrumented_model(config.model, config.output_class,\n","                              config.layer, torch.device('cuda'), use_w=config.use_w)\n","path_to_components = get_or_compute(config, inst)\n","model = inst.model\n","\n","named_directions = {} #init named_directions dict to save directions"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8NFoXruGy_C","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Load a component at random\n","\n","comps = np.load(path_to_components)\n","lst = comps.files\n","latent_dirs = []\n","latent_stdevs = []\n","\n","load_activations = False\n","\n","for item in lst:\n","\n","    if load_activations:\n","      if item == 'act_comp':\n","        for i in range(comps[item].shape[0]):\n","          latent_dirs.append(comps[item][i])\n","      if item == 'act_stdev':\n","        for i in range(comps[item].shape[0]):\n","          latent_stdevs.append(comps[item][i])\n","    else:\n","      if item == 'lat_comp':\n","        for i in range(comps[item].shape[0]):\n","          latent_dirs.append(comps[item][i])\n","      if item == 'lat_stdev':\n","        for i in range(comps[item].shape[0]):\n","          latent_stdevs.append(comps[item][i])\n","            \n","#load one at random \n","num = np.random.randint(num_components)\n","if num in named_directions.values():\n","  print(f'Direction already named: {list(named_directions.keys())[list(named_directions.values()).index(num)]}')\n","\n","random_dir = latent_dirs[num]\n","random_dir_stdev = latent_stdevs[num]\n","\n","print(f'Loaded Component No. {num}')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJytqjrVwZ7K","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Run UI\n","from ipywidgets import fixed\n","\n","# Taken from https://github.com/alexanderkuk/log-progress\n","def log_progress(sequence, every=1, size=None, name='Items'):\n","    from ipywidgets import IntProgress, HTML, VBox\n","    from IPython.display import display\n","\n","    is_iterator = False\n","    if size is None:\n","        try:\n","            size = len(sequence)\n","        except TypeError:\n","            is_iterator = True\n","    if size is not None:\n","        if every is None:\n","            if size <= 200:\n","                every = 1\n","            else:\n","                every = int(size / 200)     # every 0.5%\n","    else:\n","        assert every is not None, 'sequence is iterator, set every'\n","\n","    if is_iterator:\n","        progress = IntProgress(min=0, max=1, value=1)\n","        progress.bar_style = 'info'\n","    else:\n","        progress = IntProgress(min=0, max=size, value=0)\n","    label = HTML()\n","    box = VBox(children=[label, progress])\n","    display(box)\n","\n","    index = 0\n","    try:\n","        for index, record in enumerate(sequence, 1):\n","            if index == 1 or index % every == 0:\n","                if is_iterator:\n","                    label.value = '{name}: {index} / ?'.format(\n","                        name=name,\n","                        index=index\n","                    )\n","                else:\n","                    progress.value = index\n","                    label.value = u'{name}: {index} / {size}'.format(\n","                        name=name,\n","                        index=index,\n","                        size=size\n","                    )\n","            yield record\n","    except:\n","        progress.bar_style = 'danger'\n","        raise\n","    else:\n","        progress.bar_style = 'success'\n","        progress.value = index\n","        label.value = \"{name}: {index}\".format(\n","            name=name,\n","            index=str(index or '?')\n","        )\n","\n","def name_direction(sender):\n","  if num in named_directions.values():\n","    target_key = list(named_directions.keys())[list(named_directions.values()).index(num)]\n","    print(f'Direction already named: {target_key}')\n","    print(f'Overwriting... ')\n","    del(named_directions[target_key])\n","  named_directions[text.value] = num\n","  save_direction(random_dir, text.value)\n","  for item in named_directions:\n","    print(item, named_directions[item])\n","\n","def save_direction(direction, filename):\n","  filename += \".npy\"\n","  np.save(filename, direction, allow_pickle=True, fix_imports=True)\n","  print(f'Latent direction saved as {filename}')\n","\n","def display_sample_pytorch(seed, truncation, direction, distance, scale, start, end, disp=True, save=None, noise_spec=None):\n","    # blockPrint()\n","    with io.capture_output() as captured:\n","      w = model.sample_latent(1, seed=seed).cpu().numpy()\n","\n","      model.truncation = truncation\n","      w = [w]*model.get_max_latents() # one per layer\n","      for l in range(start, end):\n","          w[l] = w[l] + direction * distance * scale\n","\n","      #save image and display\n","      out = model.sample_np(w)\n","      final_im = Image.fromarray((out * 255).astype(np.uint8)).resize((500,500),Image.LANCZOS)\n","\n","    if disp:\n","      display(final_im)\n","    if save is not None:\n","      if disp == False:\n","        print(save)\n","      final_im.save(f'out/{seed}_{save:05}.png')\n","\n","def generate_mov(seed, truncation, direction_vec, scale, layers, n_frames, out_name = 'out', noise_spec = None, loop=True):\n","  \"\"\"Generates a mov moving back and forth along the chosen direction vector\"\"\"\n","  # Example of reading a generated set of images, and storing as MP4.\n","  %mkdir out\n","  movieName = f'out/{out_name}.mp4'\n","  offset = -10\n","  step = 20 / n_frames\n","  imgs = []\n","  for i in log_progress(range(n_frames), name = \"Generating frames\"):\n","    print(f'\\r{i} / {n_frames}', end='')\n","    w = model.sample_latent(1, seed=seed).cpu().numpy()\n","\n","    model.truncation = truncation\n","    w = [w]*model.get_max_latents() # one per layer\n","    for l in layers:\n","      if l <= model.get_max_latents():\n","          w[l] = w[l] + direction_vec * offset * scale\n","\n","    #save image and display\n","    out = model.sample_np(w)\n","    final_im = Image.fromarray((out * 255).astype(np.uint8))\n","    imgs.append(out)\n","    #increase offset\n","    offset += step\n","  if loop:\n","    imgs += imgs[::-1]\n","  with imageio.get_writer(movieName, mode='I') as writer:\n","    for image in log_progress(list(imgs), name = \"Creating animation\"):\n","        writer.append_data(img_as_ubyte(image))\n","\n","\n","seed = np.random.randint(0,100000)\n","style = {'description_width': 'initial'}\n","\n","seed = widgets.IntSlider(min=0, max=100000, step=1, value=seed, description='Seed: ', continuous_update=False)\n","truncation = widgets.FloatSlider(min=0, max=2, step=0.1, value=0.7, description='Truncation: ', continuous_update=False)\n","distance = widgets.FloatSlider(min=-10, max=10, step=0.1, value=0, description='Distance: ', continuous_update=False, style=style)\n","scale = widgets.FloatSlider(min=0, max=10, step=0.05, value=1, description='Scale: ', continuous_update=False)\n","start_layer = widgets.IntSlider(min=0, max=model.get_max_latents(), step=1, value=0, description='start layer: ', continuous_update=False)\n","end_layer = widgets.IntSlider(min=0, max=model.get_max_latents(), step=1, value=18, description='end layer: ', continuous_update=False)\n","\n","# Make sure layer range is valid\n","def update_range_start(*args):\n","  end_layer.min = start_layer.value\n","def update_range_end(*args):\n","  start_layer.max = end_layer.value\n","start_layer.observe(update_range_start, 'value')\n","end_layer.observe(update_range_end, 'value')\n","\n","text = widgets.Text(description=\"Name component here\", style=style, width=200)\n","\n","bot_box = widgets.HBox([seed, truncation, distance, scale, start_layer, end_layer, text])\n","ui = widgets.VBox([bot_box])\n","\n","out = widgets.interactive_output(display_sample_pytorch, {'seed': seed, 'truncation': truncation, 'direction': fixed(random_dir), 'distance': distance, 'scale': scale, 'start': start_layer, 'end': end_layer})\n","\n","display(ui, out)\n","text.on_submit(name_direction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJjbP91dgQui","colab_type":"code","colab":{}},"source":["#script to generate a movie moving back and forth along the direction\n","\n","direction_name = 'gender'\n","loc = named_directions[direction_name]\n","num_samples = 5\n","\n","for i in range(num_samples):\n","  s = np.random.randint(0, 10000)\n","  generate_mov(seed = s, truncation = 0.8, direction_vec = latent_dirs[loc], scale = 1.85, layers=range(4, 18), n_frames = 100, out_name = f'{model_class}_{direction_name}_{i}', loop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDbfx6e09dTX","colab_type":"code","cellView":"both","colab":{}},"source":["#@title Select from named directions\n","\n","from IPython.display import display, clear_output\n","\n","vardict = list(named_directions.keys())\n","select_variable = widgets.Dropdown(\n","    options=vardict,\n","    value=vardict[0],\n","    description='Select variable:',\n","    disabled=False,\n","    button_style=''\n",")\n","\n","def set_direction(b):\n","    clear_output()\n","    random_dir = latent_dirs[named_directions[select_variable.value]]\n","    out = widgets.interactive_output(display_sample_pytorch, {'seed': seed, 'truncation': truncation, 'direction': fixed(random_dir), 'distance': distance, 'scale': scale, 'start': start_layer, 'end': n_layers})\n","    display(select_variable)\n","    display(ui, out)\n","\n","random_dir = latent_dirs[named_directions[select_variable.value]]\n","seed = np.random.randint(0,100000)\n","style = {'description_width': 'initial'}\n","\n","seed = widgets.IntSlider(min=0, max=100000, step=1, value=seed, description='Seed: ', continuous_update=False)\n","truncation = widgets.FloatSlider(min=0, max=2, step=0.1, value=0.7, description='Truncation: ', continuous_update=False)\n","distance = widgets.FloatSlider(min=-10, max=10, step=0.1, value=0, description='Distance: ', continuous_update=False, style=style)\n","scale = widgets.FloatSlider(min=0, max=10, step=0.05, value=1, description='Scale: ', continuous_update=False)\n","start_layer = widgets.IntSlider(min=0, max=model.get_max_latents(), step=1, value=0, description='start layer: ', continuous_update=False)\n","end_layer = widgets.IntSlider(min=0, max=model.get_max_latents(), step=1, value=18, description='end layer: ', continuous_update=False)\n","\n","# Make sure layer range is valid\n","def update_range_start(*args):\n","  end_layer.min = start_layer.value\n","def update_range_end(*args):\n","  start_layer.max = end_layer.value\n","start_layer.observe(update_range_start, 'value')\n","end_layer.observe(update_range_end, 'value')\n","\n","bot_box = widgets.HBox([seed, truncation, distance, scale, start_layer, end_layer])\n","ui = widgets.VBox([bot_box])\n","out = widgets.interactive_output(display_sample_pytorch, {'seed': seed, 'truncation': truncation, 'direction': fixed(random_dir), 'distance': distance, 'scale': scale, 'start': start_layer, 'end': end_layer})\n","\n","display(select_variable)\n","display(ui, out)\n","\n","select_variable.observe(set_direction, names='value')\n","\n"],"execution_count":0,"outputs":[]}]}